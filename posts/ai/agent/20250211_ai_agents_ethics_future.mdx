---
title: AI 智能体已至，未来何去何从？[译]
date: 2025-02-11
description: 本文探讨了 AI 智能体 (AI Agents) 的兴起、定义、能力及其带来的伦理挑战。文章分析了 AI 智能体在多个方面的潜在收益与风险，包括准确性、效率、公平性、隐私和安全等。文章强调，AI 智能体自主性的提高伴随着风险的增加，需警惕完全自主智能体的潜在危险。 并简要提及了未来发展的方向。
category: ai
tags: AI, Agent, Ethics, Future, LLM, Open Source
cover: https://media.ginonotes.com/covers/hg_agent_cover.jpeg
slug: ai-agents-ethics-future
---

## 引言

大型语言模型（LLM）能力的突飞猛进 —— 例如流畅的语句表达和在各项基准测试中屡创新高 —— 促使 AI 开发者和企业纷纷展望未来：哪些具有颠覆性的技术即将到来？近期备受瞩目的一项技术便是 "AI 智能体"（AI Agents），这些系统可以在数字世界中采取与部署者目标一致的行动。如今大多数 AI 智能体都是通过将大型语言模型（LLM）整合到更大型的系统中来实现多种功能的。这股技术浪潮背后的一个基本理念是：计算机程序不再需要作为受人类控制的工具，局限于特定任务：它们现在可以在没有人为干预的情况下组合多个任务。

这一转变标志着系统发生了根本性的转变，它们能够在不确定的环境中创建针对特定情境的计划。许多现代 AI 智能体不仅仅执行预定义的操作，而是被设计用于分析新情况、制定相关目标，并采取先前未定义的操作来实现目标。

本文中，我们将简要概述什么是 AI 智能体，并详细阐述其涉及的伦理价值，讨论 AI 智能体优势与风险之间的权衡。然后，我们会提出一些发展路径，以期实现 AI 智能体最大限度地造福社会的未来。有关智能体技术方面的介绍，请参阅我们最近发布的开发者 [博客文章](https://huggingface.co/blog/smolagents)。对于现代生成式 AI 出现之前编写的智能体介绍（大部分内容在今天仍然适用），请参阅 Wooldridge 和 Jennings 于 1995 年发表的 [文章](https://core.ac.uk/reader/1498750)。

我们的分析表明，随着系统自主性的提高，对人类的风险也随之增加：用户放弃的控制权越多，系统产生的风险就越大。尤其令人担忧的是，源于 AI 智能体开发优势的个体安全风险，例如让开发者不必预测系统可能采取的所有行动。更复杂的是，一些安全危害为其他类型的危害（如隐私和安全危害）打开了大门，而对不安全系统的不当信任会导致危害进一步加剧的滚雪球效应。因此，我们建议不要开发完全自主的 AI 智能体。例如，能够编写和执行自身代码（超出开发者控制的受限代码选项）的 AI 智能体，将拥有凌驾于所有人类控制之上的能力。相比之下，半自主 AI 智能体的优势可能大于风险，具体取决于其自主性程度、系统可执行的任务以及个人控制的性质。现在，我们将深入探讨这些主题。

## 什么是 AI 智能体？

### 概述

对于 "AI 智能体" 的定义，目前尚未达成明确共识，但最近推出的 AI 智能体都有一个共同点，即它们具有 "能动性"（Agentic），也就是说，它们在某种程度上具有 *自主行动* 的能力：给定一个目标，它们可以将目标分解为子任务，并在没有人类直接干预的情况下执行每个子任务。例如，一个理想的 AI 智能体可以响应一个高级请求，如"帮我写出更好的博客文章"，通过独立地将此任务分解为：从网络上检索与您之前的博客主题相似的文章；创建包含新博客文章大纲的文档；并在每个文档中提供初步写作内容。最近在 AI 智能体方面的工作使得软件具有比以往更广泛的功能和更灵活的使用方式，最近部署的系统可以完成各种任务，从组织会议（[示例 1](https://www.lindy.ai/template-categories/meetings), [示例 2](https://www.ninjatech.ai/product/ai-scheduling-agent), [示例 3](https://attri.ai/ai-agents/scheduling-agent)）到创建个性化的社交媒体帖子（[示例](https://www.hubspot.com/products/marketing/social-media-ai-agent)），而无需明确的指示。

我们调查的所有最近推出的 AI 智能体都是建立在机器学习模型之上的，更具体地说，大多数都使用大型语言模型（LLM）来驱动其行动，这是一种全新的计算机软件方法。除了基于机器学习之外，今天的 AI 智能体与过去的智能体有相似之处，并且在某些情况下实现了 [以前关于智能体可能是什么样子的理论想法](https://core.ac.uk/download/pdf/1498750.pdf)：自主行动、展示（感知的）社交能力以及适当地平衡反应性和主动性行为。

这些特征是分级的：不同的 AI 智能体拥有不同等级的能力，可以独立工作，也可以与其他智能体协同工作以实现共同目标。因此，我们可以说 AI 智能体的自主性（或能动性）有高有低，其作为"智能体"的程度可以被视为一个连续的谱系。由于对 AI 智能体概念的理解存在差异，导致了近期出现了一些混淆和误解。本文旨在对此进行澄清。下表详细列出了 AI 智能体的不同等级。

| 能动性 | 描述 | 控制者 | 定义 | 示例代码 |
|:---:|:---|:---|:---|:---|
| ☆☆☆☆ | 模型对程序流程没有影响 | 👤 开发者控制系统可以执行的所有可能功能及其执行时间 | 简单处理器 | `print_llm_output(llm_response)` |
| ★☆☆☆ | 模型决定基本控制流程 | 👤 开发者控制系统可以执行的所有可能功能；系统控制何时执行每个功能 | 路由 | `if llm_decision(): path_a() else: path_b()` |
| ★★☆☆ | 模型决定如何执行功能 | 👤 💻 开发者控制系统可以执行的所有可能功能及其执行时间；系统控制如何执行这些功能 | 工具调用 | `run_function(llm_chosen_tool, llm_chosen_args)` |
| ★★★☆ | 模型控制迭代和程序继续 | 💻 👤 开发者控制系统可以执行的高级功能；系统控制执行哪些功能、何时执行以及如何执行 | 多步骤智能体 | `while llm_should_continue(): execute_next_step()` |
| ★★★★ | 模型编写和执行新代码 | 💻 开发者定义系统可以执行的高级功能；系统控制所有可能的功能及其执行时间 | 完全自主智能体 | `create_and_run_code(user_request)` |

**表 1.** 使用机器学习模型（例如 LLM）的系统或多或少具有能动性的一个例子。系统也可以在 "多智能体系统" 中组合，其中一个智能体工作流程触发另一个智能体工作流程，或者多个智能体协同工作以实现目标。
*改编自 [smolagent 的文章](https://huggingface.co/blog/smolagents) ，并针对本文进行了修改。*

从伦理角度来看，从人类放弃控制权并将其交给机器的角度来理解自主性的连续性也是有用的。系统越自主，我们放弃的人类控制权就越多。

在本文中，我们使用了一些拟人化的语言来描述 AI 智能体，这与当前用于描述它们的语言一致。正如 [历史学术研究](https://core.ac.uk/outputs/1498750/) 中也指出的那样，使用通常适用于人类的心理化语言（例如拥有知识、信念和意图）来描述 AI 智能体，可能会影响向用户恰当地告知系统能力。无论好坏，这种语言都是一种抽象工具，可以掩盖技术的更精确细节。在理解这些系统是什么以及它们在人们生活中可能扮演的角色时，理解这一点至关重要：使用描述 AI 智能体的心理化语言并不意味着这些系统具有思想。

### AI 智能体的光谱

AI 智能体在许多相互关联的维度上存在差异：

*   **自主性（Autonomy）：** 最近的"智能体"可以在没有用户输入的情况下至少采取一个步骤。"智能体"一词目前用于描述从单步提示和响应系统（[引文](https://blogs.microsoft.com/blog/2024/10/21/new-autonomous-agents-scale-your-team-like-never-before/)）到多步骤客户支持系统（[示例](https://www.lindy.ai/solutions/customer-support)）的所有内容。
*   **主动性（Proactivity）：** 与自主性相关的是主动性，它指的是系统在没有用户直接指定目标的情况下可以采取的目标导向行为的数量（[引文](https://core.ac.uk/outputs/1498750/)）。一个特别"主动"的 AI 智能体的例子是：一个监控你的冰箱以确定你缺少哪些食物，然后在你不知情的情况下为你购买你需要的食物的系统。智能恒温器是正在越来越多地被人们家庭采用的主动式 AI 智能体，它们会根据环境变化和它们了解到的用户行为模式自动调节温度（[示例](https://www.ecobee.com/en-us/smart-thermostats/)）。
*   **人格化（Personification）：** 可以将 AI 智能体设计为或多或少地像特定的人或人群。最近在该领域的工作（[示例 1](https://arxiv.org/abs/2411.10109), [示例 2](https://www.researchgate.net/publication/387362519_Multi-Agent_System_for_Emulating_Personality_Traits_Using_Deep_Reinforcement_Learning), [示例 3](https://medium.com/@damsa.andrei/ai-with-personality-prompting-chatgpt-using-big-five-values-def7f050462a)）侧重于根据五大性格特质——开放性、尽责性、外向性、宜人性和神经质——作为 AI 的"心理框架"（[引文](https://smythos.com/ai-agents/conversational-agents/conversational-agent-frameworks/#:~:text=The%20OCEAN%20Model%3A%20A%20Framework%20for%20Digital%20Personality&text=OCEAN%20stands%20for%20Openness%2C%20Conscientiousness,feel%20more%20authentic%20and%20relatable.)）来设计系统。在这个光谱的尽头是"数字孪生"（[示例：非智能体数字孪生](https://www.tavus.io/)）。我们目前还没有发现任何智能体数字孪生。[Salesforce 的伦理团队](https://www.salesforce.com/blog/ai-agent-design/) 等最近讨论了创建智能体数字孪生特别成问题的原因（[示例](https://www.technologyreview.com/2024/11/26/1107309/we-need-to-start-wrestling-with-the-ethics-of-ai-agents/)）。
*   **个性化（Personalization）：** AI 智能体可以使用与用户个人需求相符的语言或执行操作，例如，根据当前市场模式和用户过去进行的投资提出 [投资建议](https://www.zendesk.com/blog/ai-agents)。
*   **工具化（Tooling）：** AI 智能体还拥有不同数量的额外资源和工具。例如，第一波 AI 智能体访问搜索引擎来回答查询，随后又添加了更多工具，使它们能够操作其他技术产品，如文档和电子表格（[示例 1](https://gemini.google.com/), [示例 2](https://copilot.microsoft.com/)）。
*   **通用性（Versatility）：** 与上述能力相关的是智能体所能执行操作的多样性。这取决于以下几个方面：
    *   **领域特定性（Domain specificity）：** 智能体可以在多少个不同的领域中操作。例如，仅电子邮件，或者电子邮件与在线日历和文档一起使用。
    *   **任务特定性（Task specificity）：** 智能体可以执行多少种不同类型的任务。例如，通过在参与者的日历中创建日历邀请来安排会议（[示例](https://attri.ai/ai-agents/scheduling-agent)），或者在会议结束后向所有参与者发送有关会议的提醒电子邮件并提供会议内容的摘要（[示例](https://www.nyota.ai/)）。
    *   **模态特定性（Modality specificity）：** 智能体可以在多少种不同的模态中操作——文本、语音、视频、图像、表单、代码。一些最新的 AI 智能体被创建为高度多模态（[示例](https://deepmind.google/technologies/project-mariner/)），我们预测 AI 智能体的开发将继续增加多模态功能。
    *   **软件特定性（Software specificity）：** 智能体可以与多少种不同类型的软件交互，以及交互的深度。
*   **适应性（Adaptibility）：** 与通用性类似的是系统根据新信息或上下文变化更新其操作序列的程度。这也被描述为"动态"和"上下文感知"。
*   **操作界面（Action surfaces）：** 智能体可以执行操作的地方。传统的聊天机器人仅限于聊天界面；基于聊天的智能体还可以浏览网络并访问电子表格和文档，甚至可以通过控制计算机图形界面上的项目（例如通过移动鼠标）来执行此类任务（[示例 1](https://huggingface.co/blog/DigiRL), [示例 2](https://github.com/MinorJerry/WebVoyager), [示例 3](https://www.anthropic.com/news/3-5-models-and-computer-use)）。还有一些物理应用，例如使用模型为机器人提供动力（[示例](https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/)）。
*   **请求格式（Request formats）：** AI 智能体的一个共同主题是，用户应该能够输入要完成的任务的请求，而无需指定有关如何实现它的详细细节。这可以通过低代码解决方案（[示例](https://huggingface.co/blog/smolagents)）实现，也可以使用文本形式的人类语言或语音形式的人类语言（[示例](https://play.ai/)）实现。可以使用人类语言提供请求的 AI 智能体是最近基于 LLM 的聊天机器人取得成功的自然发展：基于聊天的"AI 智能体"比聊天机器人更进一步，因为它可以操作在聊天应用程序之外。
*   **反应性（Reactivity）：** 此特征指的是 AI 智能体完成其操作序列所需的时间：仅仅是片刻，还是更长的时间跨度。现代聊天机器人中可以看到这种效果的先兆。例如，ChatGPT 在几毫秒内响应，而 Qwen QwQ 则需要几分钟，迭代标记为"推理"的不同步骤。
*   **数量（Number）：** 系统可以是单智能体或多智能体，通过协作、按顺序或并行工作来满足用户的需求。

## 风险、收益和用途：基于价值观的分析

为了从伦理角度审视 AI 智能体，我们根据最近的 AI 智能体研究和营销中提出的不同价值观来分解它们的风险和收益。这些并非详尽无遗，而是对 AI 智能体所基于的技术（例如 LLM）已记录的风险、危害和收益的补充。我们希望本节有助于理解如何开发 AI 智能体，提供有关不同开发优先级中的收益和风险的信息。这些价值观也可能为评估协议（例如红队测试）提供信息。

**价值观：准确性（Accuracy）**

* 🙂 **潜在收益：** 通过基于可信数据，智能体可以比仅从纯模型输出操作时更准确。这可以通过基于规则的方法或机器学习方法（如 RAG）来完成，现在是为确保准确性做出新贡献的最佳时机。
* 😟 **风险：** 现代 AI 智能体的支柱是生成式 AI，它不区分真实与虚假、事实与虚构。例如，大型语言模型旨在构建看起来像流利语言的文本——这意味着它们经常产生听起来正确但非常错误的内容。应用于 AI 智能体中，LLM 输出可能导致不正确的社交媒体帖子、投资决策、会议摘要等。

**价值观：辅助性（Assistiveness）**

* 🙂 **潜在收益：** 理想情况下，智能体可以辅助用户需求，补充（而不是取代）人们。理想情况下，它们可以帮助提高用户完成任务的速度以及同时完成多个任务的效率。辅助性智能体还可以增强能力以最大限度地减少负面结果，例如帮助盲人用户在繁忙的楼梯间导航的 AI 智能体。开发良好的辅助性 AI 智能体可以为其用户提供更多的自由和机会，帮助改善用户在其组织内的积极影响，或帮助用户扩大其在公共平台上的影响力。
* 😟 **风险：** 当智能体取代人类时 —— 例如当使用 AI 智能体代替工作中的人员时 —— 这可能会导致失业和经济影响，从而进一步加剧技术创造者和为技术提供数据的人（通常未经同意）之间的差距。此外，设计不当的辅助性可能会因过度依赖或不当信任而导致危害。

**价值观：一致性（Consistency）**

关于 AI 智能体的一个讨论想法是，它们可以帮助实现一致性，因为它们比人们受周围环境的影响更小。这可能是好事也可能是坏事。我们不知道关于 AI 智能体一致性性质的严格研究，尽管相关工作表明，许多 AI 智能体所基于的 LLM 是高度不一致的（[引文 1](https://www.medrxiv.org/content/10.1101/2023.08.03.23293401v2), [引文 2](https://arxiv.org/abs/2405.01724)）。测量 AI 智能体的一致性将需要开发新的评估协议，尤其是在敏感领域。

* 🙂 **潜在收益：** AI 智能体不会像人类那样"受"世界"影响"，人类的不一致性是由情绪、饥饿、睡眠水平或对人的感知的偏见引起的（尽管 AI 智能体延续了基于它们所训练的人类内容的偏见）。多家公司都强调一致性是 AI 智能体的一个关键优势（[示例 1](https://www.salesforce.com/agentforce/what-are-ai-agents/), [示例 2](https://www.oracle.com/artificial-intelligence/ai-agents/)）。
* 😟 **风险：** 许多 AI 智能体的生成组件引入了结果的固有可变性，即使在类似情况下也是如此。这可能会影响速度和效率，因为人们必须发现并解决 AI 智能体的不当不一致性。未被注意到的不一致可能会造成安全问题。一致性也可能并不总是可取的，因为它可能与公平性相冲突。在不同的部署和操作链中保持一致性可能需要 AI 智能体记录并比较其不同的交互——这带来了监视和隐私风险。

**价值观：效率（Efficiency）**

* 🙂 **潜在收益：** AI 智能体的一个卖点是，它们可以帮助人们提高效率 —— 例如，它们会为你整理文档，以便你可以专注于花更多时间陪伴家人或从事你认为有益的工作。
* 😟 **风险：** 一个潜在的缺点是，它们可能会降低人们的效率，因为试图识别和修复智能体引入的错误（由于智能体能够采取多个连续步骤，这可能是一系列复杂的问题）可能既耗时又困难，而且压力很大。

**价值观：公平性（Equity）**

AI 智能体可能会影响情况的公平性、公正性和包容性。

* 🙂 **潜在收益：** AI 智能体可以潜在地帮助"公平竞争"。例如，会议助理可能会显示每个人有多少时间发言。这可以用来促进更平等的参与或突出性别或位置之间的不平衡（[示例](https://equaltime.io/)）。
* 😟 **风险：** 现代 AI 智能体所基于的机器学习模型是在人类数据上训练的；人类数据可能是不公平的、不公正的、排他性的，甚至更糟。由于数据收集中的样本偏差（例如，某些国家的代表性过高），也可能出现不公平的系统结果。

**价值观：类人性（Humanlikeness）**

* 🙂 **潜在收益：** 能够生成类人行为的系统提供了对不同亚群如何响应不同刺激进行模拟的机会。这在直接进行人体实验可能会造成伤害的情况下，或者当大量模拟有助于更好地解决手头的实验问题时特别有用。例如，合成人类行为可以用于预测约会兼容性，或预测经济变化和政治转变。目前正在研究的另一个潜在好处是，类人性有助于沟通甚至陪伴（[示例](https://dl.acm.org/doi/abs/10.1145/3213050)）。
* 😟 **风险（续）：** 类人性会导致用户将系统拟人化，这可能会产生负面的心理影响，例如过度依赖、不当信任、依赖和情感纠缠，从而导致反社会行为或自残。有人担心 AI 智能体的社交互动可能会导致孤独感，了解社交媒体使用可能带来的细微差别。"恐怖谷"现象增加了另一层复杂性——随着智能体变得更像人，但又无法完美地模拟人类，它们会在用户中引发不安、厌恶或认知失调的感觉。

**价值观：互通性（Interoperability）**

* 🙂 **潜在收益：** 可以与其他系统协同运行的系统可以为 AI 智能体的功能提供更大的灵活性和选择。
* 😟 **风险：** 然而，这可能会损害安全性和保障性，因为智能体越是能够影响其更有限的测试环境之外的系统并受其影响，恶意代码和意外问题的风险就越大。例如，一个连接到银行账户以便它可以轻松地代表某人购买商品的智能体，可能会耗尽银行账户。由于这种担忧，科技公司避免发布可以自主购物的 AI 智能体（[引文](https://www.wired.com/story/amazon-ai-agents-shopping-guides-rufus/)）。

**价值观：隐私（Privacy）**

* 🙂 **潜在收益：** 除了 AI 智能体提供商可监控的内容外，AI 智能体可能会提供一些隐私，以保持交易和任务完全保密。
* 😟 **风险：** 为了使智能体按照用户的期望工作，用户可能必须提供详细的个人信息，例如他们要去哪里、与谁会面以及他们在做什么。为了让智能体能够以个性化的方式代表用户行事，它还可能访问可用于提取更多私人信息（例如，从联系人列表、日历等）的应用程序和信息源。用户可以轻松地放弃对其数据（以及关于他人的私人信息）的控制以提高效率（如果信任智能体，甚至会放弃更多）；如果发生隐私泄露，AI 智能体带来的不同内容的互连可能会使情况变得更糟。例如，一个可以访问电话交谈和社交媒体帖子的 AI 智能体可以将高度私密的信息分享给全世界。

**价值观：相关性（Relevance）**

* 🙂 **潜在收益：** 创建个性化用户系统的动机之一是帮助确保其输出对用户特别相关和连贯。
* 😟 **风险：** 然而，这种个性化可能会放大现有的偏见并产生新的偏见：随着系统适应个人用户，它们可能会强化和加深现有的偏见，通过选择性信息检索产生确认偏见，并建立强化问题观点的回音室。使智能体与用户更相关的机制——它们学习和适应用户偏好的能力——可能会无意中延续和加强社会偏见，使得平衡个性化与负责任的 AI 开发的挑战变得尤为困难。

**价值观：安全（Safety）**

* 🙂 **潜在收益：** 机器人 AI 智能体可能有助于将人们从身体伤害中拯救出来，例如能够拆除炸弹、清除毒物或在对人类而言危险的环境中操作的制造业或工业环境中的智能体。
* 😟 **风险：** 智能体行为的不可预测性意味着看似安全的个体操作可能以潜在有害的方式结合起来，从而产生难以预防的新风险。（这类似于工具性趋同和回形针最大化问题。）目前还不清楚 AI 智能体是否会设计一个覆盖给定护栏的流程，或者护栏的指定方式是否会无意中产生更多问题。因此，通过更广泛的系统访问、更复杂的操作链和减少人工监督来提高智能体的能力和效率的驱动力与安全考虑因素相冲突。此外，访问广泛的界面（例如，上面"操作界面"中讨论的 GUI）和类人行为使智能体能够执行类似于具有相同控制级别的人类用户的操作，而不会触发任何警告系统——例如操纵或删除文件、在社交媒体上冒充用户，或使用存储的信用卡信息购买弹出的任何广告。AI 智能体与多个系统交互的能力以及它们可能采取的每个操作都缺乏人为监督的设计，这进一步增加了安全风险。AI 智能体可能会共同造成不安全的结果。

**价值观：科学进步（Scientific Progress）**

目前存在关于 AI 智能体是否是 AI 开发的根本性进步，还是对我们多年来拥有的技术（深度学习、启发式和管道系统）的"品牌重塑"的争论。重新引入"智能体"一词作为现代 AI 系统的总称，这些系统具有以最少的用户输入产生操作的共同特征，这是简洁地指代最近的 AI 应用程序的一种有用方法。然而，该术语带有自由和能动性的内涵，表明 AI 技术发生了更根本的变化。

本节中列出的所有价值观都与科学进步相关；其中大多数都提供了潜在收益和风险的详细信息。

**价值观：安全保障（Security）**

* 🙂 **潜在收益：** 潜在收益与隐私的潜在收益相似。
* 😟 **风险：** AI 智能体带来了严峻的安全挑战，因为它们处理通常敏感的数据（客户和用户信息）并结合了它们的安全性风险，例如与多个系统交互的能力以及它们可能采取的每个行动都缺乏人为监督的设计。即使它们的目标是由善意的用户设定的，它们也可能会共享机密信息。恶意行为者还可能劫持或操纵智能体以获得对连接系统的未经授权的访问权限、窃取敏感信息或大规模进行自动攻击。例如，可以利用可以访问电子邮件系统的智能体来共享机密数据，或者可以利用与家庭自动化集成的智能体来破坏物理安全。

**价值观：速度（Speed）**

关于用户的速度：

* 🙂 **潜在收益：** AI 智能体可以帮助用户更快地完成更多任务，充当必须完成的任务的额外帮手。
* 😟 **风险：** 然而，由于其操作中的问题，它们也可能导致更多的工作（参见效率）。

关于系统的速度：

与大多数系统一样，快速获得结果可能会以牺牲其他理想属性（如准确性、质量、低成本等）为代价。如果历史可以揭示接下来会发生什么，那么未来速度较慢的系统可能会提供更好的整体结果。

**价值观：可持续性（Sustainability）**

* 🙂 **潜在收益：** AI 智能体理论上可以帮助解决与气候变化相关的问题，例如预测野火或城市地区洪水的增长，同时分析交通模式，然后实时建议最佳路线和交通方式。未来的自动驾驶 AI 智能体可以直接做出此类路线决策，并可以与其他系统协调以获取相关更新。
* 😟 **风险：** 目前，AI 智能体所基于的机器学习模型会带来负面的环境影响，例如碳排放和饮用水的使用。更大并不总是更好，高效的硬件和低碳数据中心可以帮助减少这种情况。

**价值观：信任（Trust）**

* 🙂 **潜在收益：** 我们不知道 AI 智能体与信任相关的任何收益。系统应该被构建为值得我们信任，这意味着它们被证明是安全、可靠、有保障的等。
* 😟 **风险：** 不当信任会导致人们被操纵，以及效率、类人性和真实性方面详述的其他风险。另一个风险源于 LLM 产生虚假信息（称为"幻觉"或"虚构"）的趋势：一个在大多数情况下都是正确的系统更有可能在错误时被不当信任。

**价值观：真实性（Truthfulness）**

* 🙂 **潜在收益：** 我们不知道 AI 智能体与真实性相关的任何收益。
* 😟 **风险：** 众所周知，AI 智能体所基于的深度学习技术是虚假信息的来源，虚假信息可以以深度伪造或错误信息等形式出现。AI 智能体可用于进一步巩固此类虚假信息，例如通过收集最新信息并在多个平台上发布。这意味着 AI 智能体可用于提供关于什么是真实的什么是虚假的错误感觉、操纵人们的信念，并扩大非自愿亲密内容的影响。AI 智能体传播的虚假信息（针对特定人群进行个性化）也可用于诈骗他们。

## 建议与未来展望

AI"智能体"的当前技术水平指明了几个明确的方向：

1.  **必须设计严格的智能体评估协议。** 自动基准测试可以借鉴上面列出的 AI 智能体的不同维度。社会技术评估可以借鉴这些价值观。
2.  **必须更好地理解 AI 智能体的影响。** 应该跟踪和分析 AI 智能体的个人、组织、经济和环境影响，以便为如何进一步开发（或不开发）它们提供信息。这应该包括对 AI 智能体对福祉、社会凝聚力、就业机会、资源获取以及对气候变化的贡献的影响的分析。
3.  **必须更好地理解连锁反应。** 当一个用户部署的智能体与其他用户部署的其他智能体交互，并且它们根据彼此的输出执行操作时，目前尚不清楚它们满足用户目标的能力将如何受到影响。
4.  **必须提高透明度和披露。** 为了实现上面列出的价值观的积极影响，并最大限度地减少其负面影响，人们需要清楚地知道他们何时在与智能体交谈以及它的自主程度。清楚地披露 AI 智能体交互不仅仅需要简单的通知——它需要一种结合技术、设计和心理考虑的方法。即使当用户明确意识到他们正在与 AI 智能体交互时，他们仍然可能会体验到拟人化或产生不必要的信任。这一挑战要求透明机制在多个层面上运作：在整个交互过程中持续存在的清晰的视觉和界面提示，精心设计的对话模式定期强化智能体的非自然属性，以及在上下文中诚实地披露智能体的能力和局限性。
5.  **开源可以带来积极的影响。** 开源运动可以作为一种平衡力量，对抗 AI 智能体开发集中在少数几个强大组织手中的情况。与关于开放价值观的更广泛讨论一致，通过使智能体架构和评估协议的访问民主化，开放计划可以使更广泛的参与者参与到如何开发和部署这些系统的塑造中。这种协作方法不仅通过集体改进加速了科学进步，还有助于建立社区驱动的安全和信任标准。当智能体开发公开进行时，任何单一实体都很难为了商业利益而在相关和重要的价值观（如隐私和真实性）上妥协。开放开发中固有的透明度也创造了自然的问责制，因为社区可以验证智能体行为并确保开发与公共利益而不是狭隘的企业目标保持一致。随着智能体变得越来越复杂，其社会影响越来越大，这种开放性尤为重要。
6.  **开发者可能会创建更多能动性的"基础模型"。** 根据当前趋势和研究模式，这是可以明确预见的，而不是我们提供的与伦理相关的建议。当前的智能体技术利用了计算机科学中最近和较旧的技术集合——近期的未来研究可能会尝试将智能体模型训练为一种整体通用模型，一种多模态模型++：经过训练，可以执行操作，同时学习建模文本、图像等。

原文链接：[AI Agents Are Here. What Now?](https://huggingface.co/blog/ethics-soc-7)
