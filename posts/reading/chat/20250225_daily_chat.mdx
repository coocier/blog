---
title: 【每日一问】大语言模型应用中，向量检索技术是如何工作的？
date: 2025-02-25
description: 本文深入探讨了向量检索技术在大语言模型（LLM）应用中的重要作用。通过分析向量索引算法（如 HNSW、IVF、PQ）的特点和各类向量数据库的应用场景，为构建高性能的 LLM 应用提供技术选型建议。
category: reading
tags: Daily Chat, Vector Search, Text Embedding, Vector Database, ANN, HNSW, RAG, LLM
cover: https://media.ginonotes.com/covers/cover_daily_chat_20250225.jpeg
slug: daily-chat-20250225
---

## 引言

在与大语言模型（LLM）驱动的各种应用交互时，我们越来越多地体验到由语义理解带来的智能与便捷。这背后，向量检索技术扮演着至关重要的角色，如同一个高效的"图书管理员"，帮助 LLM 快速找到最相关的知识片段，从而给出精准的回答或生成高质量的内容。

我对向量检索技术如何支撑起 LLM 的强大能力充满好奇。这些高维的数字向量是如何被高效地组织和检索的？不同的索引算法和数据库又有哪些权衡和取舍？在构建 LLM 应用时，如何选择最适合的向量检索方案？

带着这些问题，我让 ChatGPT 进行了一番深入的调研，并整理成这份报告。

## 对话内容

![对话内容1](https://media.ginonotes.com/images/20250225_daily_chat/chatsession1.png)
![对话内容2](https://media.ginonotes.com/images/20250225_daily_chat/chatsession2.png)
![对话内容3](https://media.ginonotes.com/images/20250225_daily_chat/chatsession3.png)
![对话内容4](https://media.ginonotes.com/images/20250225_daily_chat/chatsession4.png)

## 小记

-   **向量就像文本的"DNA"：** LLM 把文本变成一串串数字（向量），就像给每个词、每句话都提取了"DNA"。这些"DNA"包含了文本的语义信息，意思相近的文本，"DNA"也更相似。

-   **向量检索就像"超级搜索引擎"：** 有了文本的"DNA"，我们就可以进行快速的相似度搜索。向量检索就像一个超级搜索引擎，但它不只看关键词，还能理解语义，找到意思相近的内容。

-   **ANN 算法是"加速神器"：** 如果要在一大堆"DNA"里找相似的，一个个比对太慢了。ANN（近似最近邻）算法就像加速神器，它用一些巧妙的方法（比如建索引），不用全部比较就能快速找到最相似的那些。

-   **HNSW、IVF、PQ，各有所长：** 常见的 ANN 算法有 HNSW、IVF、PQ 等。HNSW 速度快、精度高，但占内存多；IVF 内存占用适中，速度也还行；PQ 把向量压缩存储，省空间但精度会下降。选哪个算法，要看具体情况，就像选工具一样，没有最好，只有最合适。

-   **向量数据库是"专用仓库"：** 向量数据库是专门用来存储和检索向量的，就像一个专门存放"DNA"的仓库。常见的向量数据库有 Faiss、Weaviate、Milvus、Pinecone 等，它们各有特点，有的速度快，有的功能全，有的容易上手。

-   **传统数据库也能存向量：** 像 PostgreSQL 这样的传统数据库，也可以通过插件（pgvector）来存储和检索向量。但如果数据量特别大，或者对速度要求特别高，还是专用向量数据库更给力。

-   **混合检索是"双保险"：** 向量检索能理解语义，但有时会漏掉关键词。混合检索把关键词搜索和向量搜索结合起来，既能找到语义相关的，又能保证包含关键词，就像加了双保险。

-   **概念漂移是"与时俱进"** 词语的意思可能会随时间变化(例如一些新的网络用语)。概念漂移是指，随着时间的推移，语言或数据的含义发生了变化，导致模型变得不准确。这需要持续关注和处理， 要定期检查模型是否需要"更新"，以适应新的变化。

